{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando 3000 imágenes de intersección...\n",
      "Generando 3000 imágenes de NO intersección...\n",
      "¡Dataset generado con éxito!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURACIÓN\n",
    "# ==============================================================================\n",
    "OUTPUT_DIR = \"dataset_sintetico\"\n",
    "INTERSECTION_DIR = os.path.join(OUTPUT_DIR, \"intersections\")\n",
    "NON_INTERSECTION_DIR = os.path.join(OUTPUT_DIR, \"non_intersections\")\n",
    "\n",
    "os.makedirs(INTERSECTION_DIR, exist_ok=True)\n",
    "os.makedirs(NON_INTERSECTION_DIR, exist_ok=True)\n",
    "\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "\n",
    "N_IMAGES_INTERSECTION = 3000\n",
    "N_IMAGES_NON_INTERSECTION = 3000\n",
    "\n",
    "# ==============================================================================\n",
    "# FUNCIONES AUXILIARES\n",
    "# ==============================================================================\n",
    "def add_gaussian_noise(image, mean=0, sigma=15):\n",
    "    \"\"\"\n",
    "    Añade ruido Gaussiano a la imagen PIL.\n",
    "    El parámetro sigma controla la fuerza del ruido.\n",
    "    \"\"\"\n",
    "    img_array = np.array(image).astype(np.float32)\n",
    "    noise = np.random.normal(mean, sigma, img_array.shape)\n",
    "    noisy_img = img_array + noise\n",
    "    noisy_img = np.clip(noisy_img, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(noisy_img)\n",
    "\n",
    "def draw_bold_scaled_text(text):\n",
    "    \"\"\"\n",
    "    Crea una imagen RGBA con el texto en “negrita” y de mayor tamaño.\n",
    "    1. Dibuja el texto varias veces con offsets para simular negrita.\n",
    "    2. Escala la imagen para duplicar el tamaño.\n",
    "    Retorna la imagen RGBA resultante.\n",
    "    \"\"\"\n",
    "    # Imagen temporal para el texto (un tamaño grande para no recortar el texto)\n",
    "    temp_img = Image.new(\"RGBA\", (200, 100), (0,0,0,0))\n",
    "    temp_draw = ImageDraw.Draw(temp_img)\n",
    "    \n",
    "    # La fuente por defecto de PIL (no se puede cambiar tamaño real, \n",
    "    # pero podemos escalar la imagen después)\n",
    "    font = ImageFont.load_default()\n",
    "    \n",
    "    # Para simular \"negrita\", dibujamos el mismo texto con un pequeño offset\n",
    "    # en (0,0), (1,0), (0,1), (1,1), por ejemplo.\n",
    "    # Escogemos la posición de inicio (5,5) para no recortar caracteres altos.\n",
    "    base_x, base_y = 5, 5\n",
    "    for dx in [0, 1]:\n",
    "        for dy in [0, 1]:\n",
    "            temp_draw.text((base_x + dx, base_y + dy), text, font=font, fill=(0,0,0,255))\n",
    "\n",
    "    # Ahora medimos el bounding box real del texto (buscando píxeles no transparentes)\n",
    "    # para recortar el exceso de área transparente.\n",
    "    bbox = temp_img.getbbox()  # (left, top, right, bottom)\n",
    "    if bbox is None:\n",
    "        # Si el texto estuviera vacío, retorna la imagen tal cual\n",
    "        cropped = temp_img\n",
    "    else:\n",
    "        cropped = temp_img.crop(bbox)\n",
    "    \n",
    "    # Escalamos la imagen al doble de tamaño, para que sea “letra grande”\n",
    "    scaled_width = cropped.width * 2\n",
    "    scaled_height = cropped.height * 2\n",
    "    scaled = cropped.resize((scaled_width, scaled_height), Image.NEAREST)\n",
    "\n",
    "    return scaled\n",
    "\n",
    "def draw_rotated_text(base_image, text, x, y, angle):\n",
    "    \"\"\"\n",
    "    Genera texto “negrita y grande”, lo rota y lo pega en (x, y) sobre base_image.\n",
    "    \"\"\"\n",
    "    # Generamos la imagen con texto en negrita y escalado\n",
    "    bold_text_img = draw_bold_scaled_text(text)\n",
    "    \n",
    "    # Rotamos con expand=True para no recortar\n",
    "    rotated = bold_text_img.rotate(angle, expand=True)\n",
    "    \n",
    "    # Pegamos sobre base_image usando la propia imagen como máscara (canal alfa)\n",
    "    rx, ry = rotated.size\n",
    "    paste_box = (x, y, x + rx, y + ry)\n",
    "    base_image.paste(rotated, paste_box, rotated)\n",
    "\n",
    "def add_random_text(image):\n",
    "    \"\"\"\n",
    "    Añade texto aleatorio (letras y dígitos) en posiciones y ángulos aleatorios,\n",
    "    con “negrita y grande”.\n",
    "    \"\"\"\n",
    "    n_texts = random.randint(1, 5)\n",
    "    for _ in range(n_texts):\n",
    "        # Contenido aleatorio\n",
    "        text_length = random.randint(3, 8)\n",
    "        text_content = ''.join(random.choices(string.ascii_letters + string.digits, k=text_length))\n",
    "        \n",
    "        # Posición aleatoria\n",
    "        x_pos = random.randint(0, IMG_WIDTH - 50)\n",
    "        y_pos = random.randint(0, IMG_HEIGHT - 50)\n",
    "        \n",
    "        # Ángulo aleatorio\n",
    "        angle = random.randint(0, 359)\n",
    "        \n",
    "        draw_rotated_text(image, text_content, x_pos, y_pos, angle)\n",
    "\n",
    "def create_street_line(center_x, center_y, angle_rad):\n",
    "    \"\"\"\n",
    "    Crea los puntos (x1, y1, x2, y2) para una calle dada la posición (center_x, center_y)\n",
    "    y un ángulo en radianes.\n",
    "    \"\"\"\n",
    "    street_length = max(IMG_WIDTH, IMG_HEIGHT)\n",
    "    dx = np.cos(angle_rad)\n",
    "    dy = np.sin(angle_rad)\n",
    "    x1 = center_x - street_length * dx\n",
    "    y1 = center_y - street_length * dy\n",
    "    x2 = center_x + street_length * dx\n",
    "    y2 = center_y + street_length * dy\n",
    "    return (x1, y1, x2, y2)\n",
    "\n",
    "def draw_black_lines(draw, lines):\n",
    "    \"\"\"\n",
    "    Recibe una lista de líneas (x1,y1,x2,y2) y las dibuja en negro\n",
    "    con grosor aleatorio (15-25).\n",
    "    \"\"\"\n",
    "    thickness_black = random.randint(15, 25)\n",
    "    for (x1, y1, x2, y2) in lines:\n",
    "        draw.line((x1, y1, x2, y2), fill=\"black\", width=thickness_black)\n",
    "\n",
    "def draw_white_lines(draw, lines):\n",
    "    \"\"\"\n",
    "    Recibe la misma lista de líneas y las dibuja en blanco\n",
    "    con grosor un poco menor (8-14).\n",
    "    \"\"\"\n",
    "    thickness_white = random.randint(8, 14)\n",
    "    for (x1, y1, x2, y2) in lines:\n",
    "        draw.line((x1, y1, x2, y2), fill=\"white\", width=thickness_white)\n",
    "\n",
    "def draw_intersection(draw):\n",
    "    \"\"\"\n",
    "    Dibuja 2 o 3 calles que se cruzan en un punto cerca del centro.\n",
    "    1) Se pintan TODAS las líneas negras.\n",
    "    2) Luego TODAS las líneas blancas.\n",
    "    \"\"\"\n",
    "    center_x = random.randint(IMG_WIDTH // 3, 2 * IMG_WIDTH // 3)\n",
    "    center_y = random.randint(IMG_HEIGHT // 3, 2 * IMG_HEIGHT // 3)\n",
    "\n",
    "    # Puede cruzarse 2 o 3 calles\n",
    "    n_streets = random.choice([2, 3])\n",
    "    \n",
    "    lines = []\n",
    "    # Generamos n_streets ángulos y creamos cada línea\n",
    "    base_angle = random.randint(0, 179)\n",
    "    for i in range(n_streets):\n",
    "        # Espaciar ángulos para que no sean todos muy cercanos\n",
    "        offset = random.randint(60, 120) if i > 0 else 0\n",
    "        angle = (base_angle + offset*(i)) % 180\n",
    "        rad = np.radians(angle)\n",
    "        line = create_street_line(center_x, center_y, rad)\n",
    "        lines.append(line)\n",
    "    \n",
    "    # Primero TODAS las líneas en negro\n",
    "    draw_black_lines(draw, lines)\n",
    "    # Luego TODAS las líneas en blanco\n",
    "    draw_white_lines(draw, lines)\n",
    "\n",
    "def draw_random_non_intersection(draw):\n",
    "    \"\"\"\n",
    "    En NO intersecciones NO puede haber líneas que se crucen.\n",
    "    Por lo tanto, dibujaremos:\n",
    "       - 1 sola calle (línea) en cualquier parte\n",
    "       - o nada (espacio vacío).\n",
    "    \"\"\"\n",
    "    choice = random.choice([\"one_line\", \"nothing\"])\n",
    "    \n",
    "    if choice == \"nothing\":\n",
    "        return\n",
    "    \n",
    "    elif choice == \"one_line\":\n",
    "        cx = random.randint(0, IMG_WIDTH)\n",
    "        cy = random.randint(0, IMG_HEIGHT)\n",
    "        angle = np.radians(random.randint(0, 179))\n",
    "        line_seg = create_street_line(cx, cy, angle)\n",
    "        \n",
    "        # Pintar primero en negro, luego en blanco\n",
    "        draw_black_lines(draw, [line_seg])\n",
    "        draw_white_lines(draw, [line_seg])\n",
    "\n",
    "def generate_dataset():\n",
    "    # INTERSECTIONS\n",
    "    print(f\"Generando {N_IMAGES_INTERSECTION} imágenes de intersección...\")\n",
    "    for i in range(N_IMAGES_INTERSECTION):\n",
    "        image = Image.new(\"RGB\", (IMG_WIDTH, IMG_HEIGHT), color=\"white\")\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        # Dibuja 2 o 3 calles\n",
    "        draw_intersection(draw)\n",
    "        # Añade texto “negrita y grande” girado\n",
    "        add_random_text(image)\n",
    "        # Ruido Gaussiano\n",
    "        image = add_gaussian_noise(image)\n",
    "        \n",
    "        fname = os.path.join(INTERSECTION_DIR, f\"intersection_{i:04d}.png\")\n",
    "        image.save(fname)\n",
    "\n",
    "    # NON-INTERSECTIONS\n",
    "    print(f\"Generando {N_IMAGES_NON_INTERSECTION} imágenes de NO intersección...\")\n",
    "    for i in range(N_IMAGES_NON_INTERSECTION):\n",
    "        image = Image.new(\"RGB\", (IMG_WIDTH, IMG_HEIGHT), color=\"white\")\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        \n",
    "        # 0 o 1 línea que NO se cruce con otra\n",
    "        draw_random_non_intersection(draw)\n",
    "        # Añade texto “negrita y grande” girado\n",
    "        add_random_text(image)\n",
    "        # Ruido Gaussiano\n",
    "        image = add_gaussian_noise(image)\n",
    "        \n",
    "        fname = os.path.join(NON_INTERSECTION_DIR, f\"non_intersection_{i:04d}.png\")\n",
    "        image.save(fname)\n",
    "\n",
    "    print(\"¡Dataset generado con éxito!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BorderDetectionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(BorderDetectionCNN, self).__init__()\n",
    "        \n",
    "        # Capas convolucionales\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        \n",
    "        # BatchNorm\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Dropout para regularización\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # MaxPool 2x2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Clasificador final\n",
    "        # 224 -> 112 -> 56 -> 28 -> 14 (reducción cada pool)\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BorderDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.rotation_angles = [90, 180, 270]  # Ángulos para rotaciones adicionales\n",
    "        \n",
    "        borders_dir = os.path.join(root_dir, \"borders\")\n",
    "        for img_name in os.listdir(borders_dir):\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):\n",
    "                img_path = os.path.join(borders_dir, img_name)\n",
    "                self.samples.append((img_path, 0))\n",
    "                for angle in self.rotation_angles:\n",
    "                    self.samples.append((img_path, 0, angle))\n",
    "        \n",
    "        no_borders_dir = os.path.join(root_dir, \"no_borders\")\n",
    "        for img_name in os.listdir(no_borders_dir):\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):\n",
    "                img_path = os.path.join(no_borders_dir, img_name)\n",
    "                self.samples.append((img_path, 1))\n",
    "                # for angle in self.rotation_angles:\n",
    "                #     self.samples.append((img_path, 1, angle))\n",
    "        \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if len(self.samples[idx]) == 2:\n",
    "            # Imagen sin rotación\n",
    "            img_path, label = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        else:\n",
    "            # Imagen con rotación\n",
    "            img_path, label, angle = self.samples[idx]\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = image.rotate(angle)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "def check_data_directory(data_dir):\n",
    "    \"\"\"Verifica que existan las carpetas necesarias con las imágenes\"\"\"\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise FileNotFoundError(f\"El directorio {data_dir} no existe\")\n",
    "    \n",
    "    borders_dir = os.path.join(data_dir, \"borders\")\n",
    "    no_borders_dir = os.path.join(data_dir, \"no_borders\")\n",
    "    \n",
    "    if not os.path.exists(borders_dir):\n",
    "        raise FileNotFoundError(f\"No se encuentra la carpeta 'borders' en {data_dir}\")\n",
    "    if not os.path.exists(no_borders_dir):\n",
    "        raise FileNotFoundError(f\"No se encuentra la carpeta 'no_borders' en {data_dir}\")\n",
    "    \n",
    "    # Verificar que hay imágenes en las carpetas\n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "    borders_images = [f for f in os.listdir(borders_dir) if f.lower().endswith(valid_extensions)]\n",
    "    no_borders_images = [f for f in os.listdir(no_borders_dir) if f.lower().endswith(valid_extensions)]\n",
    "    \n",
    "    if not borders_images:\n",
    "        raise FileNotFoundError(f\"No se encontraron imágenes válidas en {borders_dir}\")\n",
    "    if not no_borders_images:\n",
    "        raise FileNotFoundError(f\"No se encontraron imágenes válidas en {no_borders_dir}\")\n",
    "    \n",
    "    print(f\"Encontradas {len(borders_images)} imágenes con bordes\")\n",
    "    print(f\"Después del data augmentation: {len(borders_images) * 4} imágenes con bordes\")\n",
    "    print(f\"Encontradas {len(no_borders_images)} imágenes sin bordes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontradas 1394 imágenes con bordes\n",
      "Después del data augmentation: 5576 imágenes con bordes\n",
      "Encontradas 5451 imágenes sin bordes\n",
      "Usando dispositivo: cuda\n",
      "Total de imágenes (incluyendo augmentation): 11027\n",
      "Imágenes de entrenamiento: 8821\n",
      "Imágenes de validación: 2206\n",
      "==> Época 1/10 <==\n",
      "  Época [1/10], Batch [10/276], Loss: 5.2493\n",
      "  Época [1/10], Batch [20/276], Loss: 1.6225\n",
      "  Época [1/10], Batch [30/276], Loss: 1.2976\n",
      "  Época [1/10], Batch [40/276], Loss: 0.7775\n",
      "  Época [1/10], Batch [50/276], Loss: 1.1146\n",
      "  Época [1/10], Batch [60/276], Loss: 0.6917\n",
      "  Época [1/10], Batch [70/276], Loss: 0.3678\n",
      "  Época [1/10], Batch [80/276], Loss: 0.3132\n",
      "  Época [1/10], Batch [90/276], Loss: 0.2705\n",
      "  Época [1/10], Batch [100/276], Loss: 0.1488\n",
      "  Época [1/10], Batch [110/276], Loss: 0.4389\n",
      "  Época [1/10], Batch [120/276], Loss: 0.4443\n",
      "  Época [1/10], Batch [130/276], Loss: 0.2338\n",
      "  Época [1/10], Batch [140/276], Loss: 0.6017\n",
      "  Época [1/10], Batch [150/276], Loss: 0.2675\n",
      "  Época [1/10], Batch [160/276], Loss: 0.2311\n",
      "  Época [1/10], Batch [170/276], Loss: 0.3710\n",
      "  Época [1/10], Batch [180/276], Loss: 0.6620\n",
      "  Época [1/10], Batch [190/276], Loss: 0.2715\n",
      "  Época [1/10], Batch [200/276], Loss: 0.1718\n",
      "  Época [1/10], Batch [210/276], Loss: 0.1836\n",
      "  Época [1/10], Batch [220/276], Loss: 0.2249\n",
      "  Época [1/10], Batch [230/276], Loss: 0.2116\n",
      "  Época [1/10], Batch [240/276], Loss: 0.1556\n",
      "  Época [1/10], Batch [250/276], Loss: 0.2503\n",
      "  Época [1/10], Batch [260/276], Loss: 0.3274\n",
      "  Época [1/10], Batch [270/276], Loss: 0.4047\n",
      "Train Loss: 1.0026 | Train Acc: 82.87%\n",
      "Val   Loss: 0.1584 | Val   Acc: 94.42%\n",
      "------------------------------------------------------------\n",
      "==> Época 2/10 <==\n",
      "  Época [2/10], Batch [10/276], Loss: 0.6617\n",
      "  Época [2/10], Batch [20/276], Loss: 0.1868\n",
      "  Época [2/10], Batch [30/276], Loss: 0.1876\n",
      "  Época [2/10], Batch [40/276], Loss: 0.3090\n",
      "  Época [2/10], Batch [50/276], Loss: 0.0974\n",
      "  Época [2/10], Batch [60/276], Loss: 0.1324\n",
      "  Época [2/10], Batch [70/276], Loss: 0.1281\n",
      "  Época [2/10], Batch [80/276], Loss: 0.2759\n",
      "  Época [2/10], Batch [90/276], Loss: 0.0464\n",
      "  Época [2/10], Batch [100/276], Loss: 0.0919\n",
      "  Época [2/10], Batch [110/276], Loss: 0.3203\n",
      "  Época [2/10], Batch [120/276], Loss: 0.1230\n",
      "  Época [2/10], Batch [130/276], Loss: 0.2753\n",
      "  Época [2/10], Batch [140/276], Loss: 0.1243\n",
      "  Época [2/10], Batch [150/276], Loss: 0.1312\n",
      "  Época [2/10], Batch [160/276], Loss: 0.0715\n",
      "  Época [2/10], Batch [170/276], Loss: 0.0947\n",
      "  Época [2/10], Batch [180/276], Loss: 0.5067\n",
      "  Época [2/10], Batch [190/276], Loss: 0.2032\n",
      "  Época [2/10], Batch [200/276], Loss: 0.4367\n",
      "  Época [2/10], Batch [210/276], Loss: 0.2815\n",
      "  Época [2/10], Batch [220/276], Loss: 0.1152\n",
      "  Época [2/10], Batch [230/276], Loss: 0.1029\n",
      "  Época [2/10], Batch [240/276], Loss: 0.0707\n",
      "  Época [2/10], Batch [250/276], Loss: 0.0550\n",
      "  Época [2/10], Batch [260/276], Loss: 0.1437\n",
      "  Época [2/10], Batch [270/276], Loss: 0.0373\n",
      "Train Loss: 0.1810 | Train Acc: 93.15%\n",
      "Val   Loss: 0.1549 | Val   Acc: 94.47%\n",
      "------------------------------------------------------------\n",
      "==> Época 3/10 <==\n",
      "  Época [3/10], Batch [10/276], Loss: 0.0720\n",
      "  Época [3/10], Batch [20/276], Loss: 0.0726\n",
      "  Época [3/10], Batch [30/276], Loss: 0.1330\n",
      "  Época [3/10], Batch [40/276], Loss: 0.1445\n",
      "  Época [3/10], Batch [50/276], Loss: 0.1446\n",
      "  Época [3/10], Batch [60/276], Loss: 0.4430\n",
      "  Época [3/10], Batch [70/276], Loss: 0.1316\n",
      "  Época [3/10], Batch [80/276], Loss: 0.3217\n",
      "  Época [3/10], Batch [90/276], Loss: 0.0756\n",
      "  Época [3/10], Batch [100/276], Loss: 0.0892\n",
      "  Época [3/10], Batch [110/276], Loss: 0.1893\n",
      "  Época [3/10], Batch [120/276], Loss: 0.0589\n",
      "  Época [3/10], Batch [130/276], Loss: 0.0969\n",
      "  Época [3/10], Batch [140/276], Loss: 0.1136\n",
      "  Época [3/10], Batch [150/276], Loss: 0.0349\n",
      "  Época [3/10], Batch [160/276], Loss: 0.0358\n",
      "  Época [3/10], Batch [170/276], Loss: 0.1743\n",
      "  Época [3/10], Batch [180/276], Loss: 0.2587\n",
      "  Época [3/10], Batch [190/276], Loss: 0.1181\n",
      "  Época [3/10], Batch [200/276], Loss: 0.0666\n",
      "  Época [3/10], Batch [210/276], Loss: 0.0505\n",
      "  Época [3/10], Batch [220/276], Loss: 0.0253\n",
      "  Época [3/10], Batch [230/276], Loss: 0.0903\n",
      "  Época [3/10], Batch [240/276], Loss: 0.0994\n",
      "  Época [3/10], Batch [250/276], Loss: 0.2826\n",
      "  Época [3/10], Batch [260/276], Loss: 0.0836\n",
      "  Época [3/10], Batch [270/276], Loss: 0.1221\n",
      "Train Loss: 0.1406 | Train Acc: 94.82%\n",
      "Val   Loss: 0.1896 | Val   Acc: 93.29%\n",
      "------------------------------------------------------------\n",
      "==> Época 4/10 <==\n",
      "  Época [4/10], Batch [10/276], Loss: 0.0666\n",
      "  Época [4/10], Batch [20/276], Loss: 0.2284\n",
      "  Época [4/10], Batch [30/276], Loss: 0.0946\n",
      "  Época [4/10], Batch [40/276], Loss: 0.0386\n",
      "  Época [4/10], Batch [50/276], Loss: 0.1676\n",
      "  Época [4/10], Batch [60/276], Loss: 0.3262\n",
      "  Época [4/10], Batch [70/276], Loss: 0.1243\n",
      "  Época [4/10], Batch [80/276], Loss: 0.1924\n",
      "  Época [4/10], Batch [90/276], Loss: 0.0263\n",
      "  Época [4/10], Batch [100/276], Loss: 0.2838\n",
      "  Época [4/10], Batch [110/276], Loss: 0.2678\n",
      "  Época [4/10], Batch [120/276], Loss: 0.0367\n",
      "  Época [4/10], Batch [130/276], Loss: 0.1019\n",
      "  Época [4/10], Batch [140/276], Loss: 0.0534\n",
      "  Época [4/10], Batch [150/276], Loss: 0.2012\n",
      "  Época [4/10], Batch [160/276], Loss: 0.0731\n",
      "  Época [4/10], Batch [170/276], Loss: 0.0163\n",
      "  Época [4/10], Batch [180/276], Loss: 0.1971\n",
      "  Época [4/10], Batch [190/276], Loss: 0.1144\n",
      "  Época [4/10], Batch [200/276], Loss: 0.0488\n",
      "  Época [4/10], Batch [210/276], Loss: 0.0479\n",
      "  Época [4/10], Batch [220/276], Loss: 0.4011\n",
      "  Época [4/10], Batch [230/276], Loss: 0.0338\n",
      "  Época [4/10], Batch [240/276], Loss: 0.0353\n",
      "  Época [4/10], Batch [250/276], Loss: 0.0303\n",
      "  Época [4/10], Batch [260/276], Loss: 0.1081\n",
      "  Época [4/10], Batch [270/276], Loss: 0.1827\n",
      "Train Loss: 0.1194 | Train Acc: 95.92%\n",
      "Val   Loss: 0.1221 | Val   Acc: 96.37%\n",
      "------------------------------------------------------------\n",
      "==> Época 5/10 <==\n",
      "  Época [5/10], Batch [10/276], Loss: 0.1958\n",
      "  Época [5/10], Batch [20/276], Loss: 0.0991\n",
      "  Época [5/10], Batch [30/276], Loss: 0.2355\n",
      "  Época [5/10], Batch [40/276], Loss: 0.0982\n",
      "  Época [5/10], Batch [50/276], Loss: 0.0864\n",
      "  Época [5/10], Batch [60/276], Loss: 0.0128\n",
      "  Época [5/10], Batch [70/276], Loss: 0.0296\n",
      "  Época [5/10], Batch [80/276], Loss: 0.0995\n",
      "  Época [5/10], Batch [90/276], Loss: 0.0997\n",
      "  Época [5/10], Batch [100/276], Loss: 0.5112\n",
      "  Época [5/10], Batch [110/276], Loss: 0.0314\n",
      "  Época [5/10], Batch [120/276], Loss: 0.0290\n",
      "  Época [5/10], Batch [130/276], Loss: 0.0926\n",
      "  Época [5/10], Batch [140/276], Loss: 0.2461\n",
      "  Época [5/10], Batch [150/276], Loss: 0.1310\n",
      "  Época [5/10], Batch [160/276], Loss: 0.1251\n",
      "  Época [5/10], Batch [170/276], Loss: 0.0899\n",
      "  Época [5/10], Batch [180/276], Loss: 0.2244\n",
      "  Época [5/10], Batch [190/276], Loss: 0.0663\n",
      "  Época [5/10], Batch [200/276], Loss: 0.0751\n",
      "  Época [5/10], Batch [210/276], Loss: 0.1671\n",
      "  Época [5/10], Batch [220/276], Loss: 0.0206\n",
      "  Época [5/10], Batch [230/276], Loss: 0.0959\n",
      "  Época [5/10], Batch [240/276], Loss: 0.0136\n",
      "  Época [5/10], Batch [250/276], Loss: 0.0268\n",
      "  Época [5/10], Batch [260/276], Loss: 0.2638\n",
      "  Época [5/10], Batch [270/276], Loss: 0.1211\n",
      "Train Loss: 0.1059 | Train Acc: 96.33%\n",
      "Val   Loss: 0.0457 | Val   Acc: 98.46%\n",
      "------------------------------------------------------------\n",
      "==> Época 6/10 <==\n",
      "  Época [6/10], Batch [10/276], Loss: 0.1578\n",
      "  Época [6/10], Batch [20/276], Loss: 0.3197\n",
      "  Época [6/10], Batch [30/276], Loss: 0.0612\n",
      "  Época [6/10], Batch [40/276], Loss: 0.0338\n",
      "  Época [6/10], Batch [50/276], Loss: 0.0308\n",
      "  Época [6/10], Batch [60/276], Loss: 0.1053\n",
      "  Época [6/10], Batch [70/276], Loss: 0.0917\n",
      "  Época [6/10], Batch [80/276], Loss: 0.1156\n",
      "  Época [6/10], Batch [90/276], Loss: 0.0936\n",
      "  Época [6/10], Batch [100/276], Loss: 0.2099\n",
      "  Época [6/10], Batch [110/276], Loss: 0.2547\n",
      "  Época [6/10], Batch [120/276], Loss: 0.0161\n",
      "  Época [6/10], Batch [130/276], Loss: 0.0201\n",
      "  Época [6/10], Batch [140/276], Loss: 0.1782\n",
      "  Época [6/10], Batch [150/276], Loss: 0.0413\n",
      "  Época [6/10], Batch [160/276], Loss: 0.0438\n",
      "  Época [6/10], Batch [170/276], Loss: 0.1201\n",
      "  Época [6/10], Batch [180/276], Loss: 0.0868\n",
      "  Época [6/10], Batch [190/276], Loss: 0.0909\n",
      "  Época [6/10], Batch [200/276], Loss: 0.0213\n",
      "  Época [6/10], Batch [210/276], Loss: 0.0907\n",
      "  Época [6/10], Batch [220/276], Loss: 0.1773\n",
      "  Época [6/10], Batch [230/276], Loss: 0.1868\n",
      "  Época [6/10], Batch [240/276], Loss: 0.0809\n",
      "  Época [6/10], Batch [250/276], Loss: 0.0531\n",
      "  Época [6/10], Batch [260/276], Loss: 0.0175\n",
      "  Época [6/10], Batch [270/276], Loss: 0.7111\n",
      "Train Loss: 0.1066 | Train Acc: 96.61%\n",
      "Val   Loss: 0.0736 | Val   Acc: 97.51%\n",
      "------------------------------------------------------------\n",
      "==> Época 7/10 <==\n",
      "  Época [7/10], Batch [10/276], Loss: 0.0638\n",
      "  Época [7/10], Batch [20/276], Loss: 0.2842\n",
      "  Época [7/10], Batch [30/276], Loss: 0.0995\n",
      "  Época [7/10], Batch [40/276], Loss: 0.1098\n",
      "  Época [7/10], Batch [50/276], Loss: 0.2892\n",
      "  Época [7/10], Batch [60/276], Loss: 0.0396\n",
      "  Época [7/10], Batch [70/276], Loss: 0.0912\n",
      "  Época [7/10], Batch [80/276], Loss: 0.0183\n",
      "  Época [7/10], Batch [90/276], Loss: 0.0113\n",
      "  Época [7/10], Batch [100/276], Loss: 0.0384\n",
      "  Época [7/10], Batch [110/276], Loss: 0.1588\n",
      "  Época [7/10], Batch [120/276], Loss: 0.1275\n",
      "  Época [7/10], Batch [130/276], Loss: 0.0391\n",
      "  Época [7/10], Batch [140/276], Loss: 0.0276\n",
      "  Época [7/10], Batch [150/276], Loss: 0.0580\n",
      "  Época [7/10], Batch [160/276], Loss: 0.0147\n",
      "  Época [7/10], Batch [170/276], Loss: 0.1211\n",
      "  Época [7/10], Batch [180/276], Loss: 0.0432\n",
      "  Época [7/10], Batch [190/276], Loss: 0.0345\n",
      "  Época [7/10], Batch [200/276], Loss: 0.0201\n",
      "  Época [7/10], Batch [210/276], Loss: 0.1504\n",
      "  Época [7/10], Batch [220/276], Loss: 0.1449\n",
      "  Época [7/10], Batch [230/276], Loss: 0.0601\n",
      "  Época [7/10], Batch [240/276], Loss: 0.0504\n",
      "  Época [7/10], Batch [250/276], Loss: 0.0251\n",
      "  Época [7/10], Batch [260/276], Loss: 0.0062\n",
      "  Época [7/10], Batch [270/276], Loss: 0.0926\n",
      "Train Loss: 0.0739 | Train Acc: 97.27%\n",
      "Val   Loss: 0.0898 | Val   Acc: 97.23%\n",
      "------------------------------------------------------------\n",
      "==> Época 8/10 <==\n",
      "  Época [8/10], Batch [10/276], Loss: 0.0766\n",
      "  Época [8/10], Batch [20/276], Loss: 0.0276\n",
      "  Época [8/10], Batch [30/276], Loss: 0.3631\n",
      "  Época [8/10], Batch [40/276], Loss: 0.1140\n",
      "  Época [8/10], Batch [50/276], Loss: 0.1858\n",
      "  Época [8/10], Batch [60/276], Loss: 0.0234\n",
      "  Época [8/10], Batch [70/276], Loss: 0.0503\n",
      "  Época [8/10], Batch [80/276], Loss: 0.0635\n",
      "  Época [8/10], Batch [90/276], Loss: 0.3162\n",
      "  Época [8/10], Batch [100/276], Loss: 0.0566\n",
      "  Época [8/10], Batch [110/276], Loss: 0.1425\n",
      "  Época [8/10], Batch [120/276], Loss: 0.0424\n",
      "  Época [8/10], Batch [130/276], Loss: 0.0859\n",
      "  Época [8/10], Batch [140/276], Loss: 0.0287\n",
      "  Época [8/10], Batch [150/276], Loss: 0.0260\n",
      "  Época [8/10], Batch [160/276], Loss: 0.1453\n",
      "  Época [8/10], Batch [170/276], Loss: 0.1823\n",
      "  Época [8/10], Batch [180/276], Loss: 0.0109\n",
      "  Época [8/10], Batch [190/276], Loss: 0.0430\n",
      "  Época [8/10], Batch [200/276], Loss: 0.1441\n",
      "  Época [8/10], Batch [210/276], Loss: 0.0380\n",
      "  Época [8/10], Batch [220/276], Loss: 0.2629\n",
      "  Época [8/10], Batch [230/276], Loss: 0.2521\n",
      "  Época [8/10], Batch [240/276], Loss: 0.0388\n",
      "  Época [8/10], Batch [250/276], Loss: 0.0516\n",
      "  Época [8/10], Batch [260/276], Loss: 0.0253\n",
      "  Época [8/10], Batch [270/276], Loss: 0.0244\n",
      "Train Loss: 0.0919 | Train Acc: 96.93%\n",
      "Val   Loss: 0.0458 | Val   Acc: 98.37%\n",
      "------------------------------------------------------------\n",
      "==> Época 9/10 <==\n",
      "  Época [9/10], Batch [10/276], Loss: 0.0569\n",
      "  Época [9/10], Batch [20/276], Loss: 0.1530\n",
      "  Época [9/10], Batch [30/276], Loss: 0.0347\n",
      "  Época [9/10], Batch [40/276], Loss: 0.0460\n",
      "  Época [9/10], Batch [50/276], Loss: 0.0516\n",
      "  Época [9/10], Batch [60/276], Loss: 0.0108\n",
      "  Época [9/10], Batch [70/276], Loss: 0.0170\n",
      "  Época [9/10], Batch [80/276], Loss: 0.0134\n",
      "  Época [9/10], Batch [90/276], Loss: 0.0403\n",
      "  Época [9/10], Batch [100/276], Loss: 0.0226\n",
      "  Época [9/10], Batch [110/276], Loss: 0.0343\n",
      "  Época [9/10], Batch [120/276], Loss: 0.0533\n",
      "  Época [9/10], Batch [130/276], Loss: 0.0182\n",
      "  Época [9/10], Batch [140/276], Loss: 0.1144\n",
      "  Época [9/10], Batch [150/276], Loss: 0.0320\n",
      "  Época [9/10], Batch [160/276], Loss: 0.0016\n",
      "  Época [9/10], Batch [170/276], Loss: 0.0229\n",
      "  Época [9/10], Batch [180/276], Loss: 0.0465\n",
      "  Época [9/10], Batch [190/276], Loss: 0.0633\n",
      "  Época [9/10], Batch [200/276], Loss: 0.0146\n",
      "  Época [9/10], Batch [210/276], Loss: 0.0266\n",
      "  Época [9/10], Batch [220/276], Loss: 0.0043\n",
      "  Época [9/10], Batch [230/276], Loss: 0.0165\n",
      "  Época [9/10], Batch [240/276], Loss: 0.0406\n",
      "  Época [9/10], Batch [250/276], Loss: 0.0016\n",
      "  Época [9/10], Batch [260/276], Loss: 0.0601\n",
      "  Época [9/10], Batch [270/276], Loss: 0.0671\n",
      "Train Loss: 0.0484 | Train Acc: 98.36%\n",
      "Val   Loss: 0.0318 | Val   Acc: 99.09%\n",
      "------------------------------------------------------------\n",
      "==> Época 10/10 <==\n",
      "  Época [10/10], Batch [10/276], Loss: 0.0380\n",
      "  Época [10/10], Batch [20/276], Loss: 0.0655\n",
      "  Época [10/10], Batch [30/276], Loss: 0.0460\n",
      "  Época [10/10], Batch [40/276], Loss: 0.0036\n",
      "  Época [10/10], Batch [50/276], Loss: 0.0051\n",
      "  Época [10/10], Batch [60/276], Loss: 0.0010\n",
      "  Época [10/10], Batch [70/276], Loss: 0.0321\n",
      "  Época [10/10], Batch [80/276], Loss: 0.0140\n",
      "  Época [10/10], Batch [90/276], Loss: 0.0328\n",
      "  Época [10/10], Batch [100/276], Loss: 0.0033\n",
      "  Época [10/10], Batch [110/276], Loss: 0.0208\n",
      "  Época [10/10], Batch [120/276], Loss: 0.0790\n",
      "  Época [10/10], Batch [130/276], Loss: 0.0015\n",
      "  Época [10/10], Batch [140/276], Loss: 0.0626\n",
      "  Época [10/10], Batch [150/276], Loss: 0.0058\n",
      "  Época [10/10], Batch [160/276], Loss: 0.0917\n",
      "  Época [10/10], Batch [170/276], Loss: 0.0102\n",
      "  Época [10/10], Batch [180/276], Loss: 0.0134\n",
      "  Época [10/10], Batch [190/276], Loss: 0.0115\n",
      "  Época [10/10], Batch [200/276], Loss: 0.0678\n",
      "  Época [10/10], Batch [210/276], Loss: 0.1151\n",
      "  Época [10/10], Batch [220/276], Loss: 0.0179\n",
      "  Época [10/10], Batch [230/276], Loss: 0.0344\n",
      "  Época [10/10], Batch [240/276], Loss: 0.0056\n",
      "  Época [10/10], Batch [250/276], Loss: 0.0010\n",
      "  Época [10/10], Batch [260/276], Loss: 0.0026\n",
      "  Época [10/10], Batch [270/276], Loss: 0.0042\n",
      "Train Loss: 0.0341 | Train Acc: 98.83%\n",
      "Val   Loss: 0.0331 | Val   Acc: 99.09%\n",
      "------------------------------------------------------------\n",
      "Entrenamiento finalizado en 399.73 segundos.\n",
      "Mejor modelo guardado como 'mejor_modelo_bordes.pth'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # ==============================================================================\n",
    "    # CONFIGURACIÓN BÁSICA\n",
    "    # ==============================================================================\n",
    "    DATASET_DIR = \"dataset_leo_synth\"\n",
    "    DATA_DIR = os.path.abspath(DATASET_DIR)\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 10\n",
    "    LEARNING_RATE = 1e-3\n",
    "    \n",
    "    # Verificar la estructura del directorio y las imágenes\n",
    "    try:\n",
    "        check_data_directory(DATA_DIR)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error al cargar los datos: {str(e)}\")\n",
    "        print(\"\\nAsegúrate de que tu estructura de directorios sea así:\")\n",
    "        print(f\"{DATASET_DIR}/\")\n",
    "        print(\"├── borders/\")\n",
    "        print(\"│   ├── imagen1.jpg\")\n",
    "        print(\"│   └── ...\")\n",
    "        print(\"└── no_borders/\")\n",
    "        print(\"    ├── imagen1.jpg\")\n",
    "        print(\"    └── ...\")\n",
    "        return\n",
    "\n",
    "    # Dispositivo: GPU si está disponible, sino CPU\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Usando dispositivo: {DEVICE}\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # TRANSFORMS Y DATASET\n",
    "    # ==============================================================================\n",
    "    data_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Usar nuestro dataset personalizado\n",
    "    full_dataset = BorderDataset(DATA_DIR, transform=data_transforms)\n",
    "    \n",
    "    # Separar en train y valid (80% - 20%)\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    val_size = total_size - train_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    # Ajustar num_workers a 0 para evitar bloqueos (sobre todo en Windows)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"Total de imágenes (incluyendo augmentation): {total_size}\")\n",
    "    print(f\"Imágenes de entrenamiento: {train_size}\")\n",
    "    print(f\"Imágenes de validación: {val_size}\")\n",
    "\n",
    "    # Instanciar el modelo y moverlo a GPU/CPU\n",
    "    model = BorderDetectionCNN(num_classes=2).to(DEVICE)\n",
    "\n",
    "    # Definir función de pérdida y optimizador\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Scheduler para ajuste dinámico del LR (opcional)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
    "\n",
    "    # ==============================================================================\n",
    "    # ENTRENAMIENTO\n",
    "    # ==============================================================================\n",
    "\n",
    "    # -- Inicializar listas para graficar\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "\n",
    "    def train_one_epoch(epoch):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f'  Época [{epoch+1}/{EPOCHS}], Batch [{i+1}/{len(train_loader)}], '\n",
    "                      f'Loss: {loss.item():.4f}')\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100.0 * correct / total\n",
    "        return epoch_loss, epoch_acc\n",
    "\n",
    "    def validate_one_epoch():\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_loss = running_loss / len(val_loader)\n",
    "        val_acc = 100.0 * correct / total\n",
    "        return val_loss, val_acc\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"==> Época {epoch+1}/{EPOCHS} <==\")\n",
    "        train_loss, train_acc = train_one_epoch(epoch)\n",
    "        val_loss, val_acc = validate_one_epoch()\n",
    "        \n",
    "        # Guardar resultados\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        # Actualizar learning rate con el scheduler (si se usa)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Guardar el mejor modelo (según val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"mejor_modelo_bordes.pth\")\n",
    "\n",
    "        # Mostrar métricas de la época\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.2f}%\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Entrenamiento finalizado en {total_time:.2f} segundos.\")\n",
    "    print(f\"Mejor modelo guardado como 'mejor_modelo_bordes.pth'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_11936\\2628937369.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_11936\\2628937369.py:120: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap = cm.get_cmap('jet')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap guardado en: habana_heatmap.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.cm as cm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# IMPORTANTE:\n",
    "# Asegúrate de haber definido o importado tu modelo, por ejemplo:\n",
    "#   from tu_archivo_modelo import BorderDetectionCNN\n",
    "# Aquí se asume que lo tienes disponible en el entorno.\n",
    "\n",
    "def generate_heatmap(\n",
    "    image_path: str,\n",
    "    model_path: str = \"mejor_modelo_bordes.pth\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Genera un heatmap de probabilidad de borde usando un modelo entrenado y un \n",
    "    enfoque de ventana deslizante. Devuelve el heatmap como matriz [0,1] y\n",
    "    guarda la visualización superpuesta con sufijo '_heatmap.png'.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    - image_path: Ruta de la imagen de entrada.\n",
    "    - model_path: Ruta al modelo (checkpoint) entrenado.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    - heatmap_normalized: np.ndarray con valores [0,1], mismo tamaño que la imagen original.\n",
    "    \"\"\"\n",
    "    \n",
    "    #--------------------------------------------------------------------------\n",
    "    # 1. Cargar el modelo y moverlo a GPU/CPU\n",
    "    #--------------------------------------------------------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BorderDetectionCNN(num_classes=2)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 2. Leer la imagen en color (RGB)\n",
    "    #   * Sin convertir a escala de grises, para evitar discrepancias\n",
    "    #--------------------------------------------------------------------------\n",
    "    original_image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_array = np.array(original_image)  # (alto, ancho, 3)\n",
    "    height, width, _ = img_array.shape\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 3. Definir tamaño y paso de la ventana deslizante\n",
    "    #--------------------------------------------------------------------------\n",
    "    WINDOW_SIZE = height // 75\n",
    "    if WINDOW_SIZE < 2:\n",
    "        WINDOW_SIZE = 2\n",
    "\n",
    "    STEP_SIZE = WINDOW_SIZE // 2\n",
    "    if STEP_SIZE < 1:\n",
    "        STEP_SIZE = 1\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 4. Inicializar heatmap y mapa de conteo\n",
    "    #--------------------------------------------------------------------------\n",
    "    heatmap = np.zeros((height, width), dtype=np.float32)\n",
    "    count_map = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 5. Transformaciones de entrada (mismas que en entrenamiento)\n",
    "    #--------------------------------------------------------------------------\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std =[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 6. Recorrer la imagen con la ventana deslizante\n",
    "    #--------------------------------------------------------------------------\n",
    "    for y in range(0, height - WINDOW_SIZE, STEP_SIZE):\n",
    "        for x in range(0, width - WINDOW_SIZE, STEP_SIZE):\n",
    "            # Extraer el patch\n",
    "            patch_3ch = img_array[y : y + WINDOW_SIZE, x : x + WINDOW_SIZE, :]\n",
    "\n",
    "            # Transformar al tensor\n",
    "            patch_tensor = transform(patch_3ch).unsqueeze(0).to(device)\n",
    "\n",
    "            # Inferencia\n",
    "            with torch.no_grad():\n",
    "                logits = model(patch_tensor)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "            # Asumiendo que clase 0 = \"no_borde\" y clase 1 = \"borde\",\n",
    "            # prob_border es la probabilidad de la clase 1\n",
    "            prob_border = probs[0, 1].item()\n",
    "\n",
    "            # Sumar la probabilidad en la región correspondiente\n",
    "            heatmap[y : y + WINDOW_SIZE, x : x + WINDOW_SIZE] += prob_border\n",
    "            count_map[y : y + WINDOW_SIZE, x : x + WINDOW_SIZE] += 1\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 7. Normalizar el heatmap [0,1]\n",
    "    #--------------------------------------------------------------------------\n",
    "    count_map = np.maximum(count_map, 1e-5)  # evitar división por cero\n",
    "    heatmap /= count_map  # promedio de las probabilidades\n",
    "    h_min, h_max = heatmap.min(), heatmap.max()\n",
    "    heatmap_normalized = (heatmap - h_min) / (h_max - h_min + 1e-8)\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 8. Generar visualización superpuesta y guardarla\n",
    "    #--------------------------------------------------------------------------\n",
    "    # Convertir de RGB a BGR para usar cv2.addWeighted correctamente\n",
    "    base_img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    cmap = cm.get_cmap('jet')\n",
    "    heatmap_color = cmap(heatmap_normalized)[..., :3]  # quitar canal alpha\n",
    "    heatmap_color = (heatmap_color * 255).astype(np.uint8)\n",
    "\n",
    "    alpha = 0.5  # transparencia\n",
    "    overlay = cv2.addWeighted(base_img_bgr, 1.0 - alpha, heatmap_color, alpha, 0)\n",
    "\n",
    "    # Guardar resultado\n",
    "    base_name, _ = os.path.splitext(os.path.basename(image_path))\n",
    "    output_filename = f\"{base_name}_heatmap.png\"\n",
    "    cv2.imwrite(output_filename, overlay)\n",
    "    print(f\"Heatmap guardado en: {output_filename}\")\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 9. Retornar el heatmap normalizado como np.ndarray\n",
    "    #--------------------------------------------------------------------------\n",
    "    return heatmap_normalized\n",
    "\n",
    "heatmap_mat = generate_heatmap(\"habana.jpg\", \"mejor_modelo_bordes.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Implementación de Grad-CAM para PyTorch.\n",
    "    \n",
    "    Uso:\n",
    "    ----\n",
    "    1) Instanciar: gradcam = GradCAM(model, target_layer=model.conv4)\n",
    "    2) Forward:    output = gradcam.forward(input_tensor)\n",
    "    3) Backward:   output[:, class_idx].backward(retain_graph=True)\n",
    "    4) Generar:    cam = gradcam.generate(class_idx=class_idx)\n",
    "       ( 'cam' será una lista de mapas [batch_size] con valores [0,1] )\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layer):\n",
    "        \"\"\"\n",
    "        :param model: El modelo (p.e. instancia de BorderDetectionCNN).\n",
    "        :param target_layer: Capa (módulo) del modelo donde enganchar hooks.\n",
    "                             E.g. model.conv4\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        \n",
    "        # Aquí guardaremos la activación y el gradiente de esa capa\n",
    "        self.activation = None\n",
    "        self.gradient = None\n",
    "        \n",
    "        # Registrar hooks\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        \"\"\"\n",
    "        Registra hooks para guardar la activación (forward) \n",
    "        y el gradiente (backward) de la capa objetivo.\n",
    "        \"\"\"\n",
    "        def forward_hook(module, input, output):\n",
    "            # Activación de la capa\n",
    "            self.activation = output.detach()\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            # grad_out[0] es el gradiente con respecto a la salida de la capa\n",
    "            self.gradient = grad_out[0].detach()\n",
    "\n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_backward_hook(backward_hook)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Ejecuta forward en el modelo completo y devuelve la salida (logits).\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "    \n",
    "    def generate(self, class_idx, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Genera el mapa de Grad-CAM para la clase 'class_idx' \n",
    "        usando la activación y gradiente guardados por los hooks.\n",
    "        \n",
    "        :param class_idx: Índice de la clase objetivo (p.e. 1 para \"borde\").\n",
    "        :return: Lista de CAMs (np.array) normalizados [0,1], uno por \n",
    "                 cada elemento en el batch (forma [batch_size, h, w]).\n",
    "        \"\"\"\n",
    "        # La forma de self.activation es [B, C, H, W]\n",
    "        # La forma de self.gradient   es [B, C, H, W]\n",
    "        \n",
    "        # 1) Promediar gradiente espacialmente: alpha_k = mean(grad_k)\n",
    "        #    (Promedio por cada canal k)\n",
    "        alpha = self.gradient.view(self.gradient.size(0), \n",
    "                                   self.gradient.size(1), -1).mean(dim=2) \n",
    "        # alpha shape: [B, C]\n",
    "\n",
    "        # 2) Ponderar la activación por alpha\n",
    "        #    Expandir alpha para que sea [B, C, 1, 1]\n",
    "        alpha = alpha.unsqueeze(-1).unsqueeze(-1)  \n",
    "        weighted_activation = alpha * self.activation\n",
    "        \n",
    "        # 3) Sumar canales: CAM = ReLU( sum_k( alpha_k * A_k ) )\n",
    "        cam = weighted_activation.sum(dim=1, keepdim=True)  # [B,1,H,W]\n",
    "        cam = F.relu(cam)\n",
    "        \n",
    "        # 4) Normalizar cada CAM individualmente a [0,1]\n",
    "        cams_result = []\n",
    "        for i in range(cam.size(0)):\n",
    "            # cam[i] -> [1, H, W]\n",
    "            single_cam = cam[i, 0, :, :].cpu().numpy()\n",
    "            min_v, max_v = single_cam.min(), single_cam.max()\n",
    "            single_cam = (single_cam - min_v) / (max_v - min_v + eps)\n",
    "            cams_result.append(single_cam)\n",
    "        \n",
    "        return cams_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_11936\\3074028824.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GradCAM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 166\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gradcam_map_norm\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Ejemplo de uso:\u001b[39;00m\n\u001b[1;32m--> 166\u001b[0m gradcam_map \u001b[38;5;241m=\u001b[39m generate_heatmap_gradcam(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhabana.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmejor_modelo_bordes.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 37\u001b[0m, in \u001b[0;36mgenerate_heatmap_gradcam\u001b[1;34m(image_path, model_path)\u001b[0m\n\u001b[0;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#--------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 2. Definir Grad-CAM en la capa deseada (e.g. conv4)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#--------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m gradcam \u001b[38;5;241m=\u001b[39m GradCAM(model, target_layer\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconv4)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m#--------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# 3. Leer la imagen en color (RGB)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m#--------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     42\u001b[0m original_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GradCAM' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.cm as cm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# Asegúrate de haber definido tu modelo (BorderDetectionCNN) y de tener el GradCAM definido\n",
    "# from tu_archivo_modelo import BorderDetectionCNN\n",
    "# from tu_archivo_gradcam import GradCAM\n",
    "\n",
    "def generate_heatmap_gradcam(\n",
    "    image_path: str,\n",
    "    model_path: str = \"mejor_modelo_bordes.pth\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Genera un \"heatmap\" usando Grad-CAM por ventanas deslizantes.\n",
    "    Se superpone cada CAM local al patch correspondiente en la imagen global.\n",
    "    Finalmente, guarda y retorna el mapa normalizado (0,1).\n",
    "    \"\"\"\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 1. Cargar el modelo y moverlo a GPU/CPU\n",
    "    #--------------------------------------------------------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BorderDetectionCNN(num_classes=2)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 2. Definir Grad-CAM en la capa deseada (e.g. conv4)\n",
    "    #--------------------------------------------------------------------------\n",
    "    gradcam = GradCAM(model, target_layer=model.conv4)\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 3. Leer la imagen en color (RGB)\n",
    "    #--------------------------------------------------------------------------\n",
    "    original_image = Image.open(image_path).convert(\"RGB\")\n",
    "    img_array = np.array(original_image)  # (alto, ancho, 3)\n",
    "    height, width, _ = img_array.shape\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 4. Definir tamaño y paso de la ventana deslizante\n",
    "    #--------------------------------------------------------------------------\n",
    "    WINDOW_SIZE = height // 75\n",
    "    if WINDOW_SIZE < 2:\n",
    "        WINDOW_SIZE = 2\n",
    "\n",
    "    STEP_SIZE = WINDOW_SIZE // 2\n",
    "    if STEP_SIZE < 1:\n",
    "        STEP_SIZE = 1\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 5. Inicializar un mapa para Grad-CAM y un mapa de conteo\n",
    "    #--------------------------------------------------------------------------\n",
    "    gradcam_map = np.zeros((height, width), dtype=np.float32)\n",
    "    count_map = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 6. Transformaciones de entrada (mismas que en entrenamiento)\n",
    "    #--------------------------------------------------------------------------\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std =[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 7. Recorrer la imagen con la ventana deslizante\n",
    "    #--------------------------------------------------------------------------\n",
    "    for y in range(0, height - WINDOW_SIZE, STEP_SIZE):\n",
    "        for x in range(0, width - WINDOW_SIZE, STEP_SIZE):\n",
    "            # Extraer el patch original\n",
    "            patch_3ch = img_array[y : y + WINDOW_SIZE, x : x + WINDOW_SIZE, :]\n",
    "\n",
    "            # Transformar al tensor (224x224)\n",
    "            patch_tensor = transform(patch_3ch).unsqueeze(0).to(device)\n",
    "            \n",
    "            #------------------------------------------------------------------\n",
    "            # (a) Forward del modelo a través de Grad-CAM\n",
    "            #------------------------------------------------------------------\n",
    "            model.zero_grad()\n",
    "            output = gradcam.forward(patch_tensor)  # logits -> [1,2]\n",
    "            \n",
    "            # Aquí elegimos la clase \"borde\" (índice 1) o podrías usar la clase\n",
    "            # predicha: class_idx = output.argmax(dim=1).item()\n",
    "            class_idx = 1\n",
    "\n",
    "            #------------------------------------------------------------------\n",
    "            # (b) Backward para Grad-CAM (solo de la clase_idx)\n",
    "            #------------------------------------------------------------------\n",
    "            target = output[:, class_idx]\n",
    "            target.backward(retain_graph=True)\n",
    "            \n",
    "            #------------------------------------------------------------------\n",
    "            # (c) Generar la cam normalizada [0,1]\n",
    "            #------------------------------------------------------------------\n",
    "            cam_list = gradcam.generate(class_idx=class_idx)\n",
    "            cam_patch = cam_list[0]  # batch_size=1, tomamos el primero\n",
    "\n",
    "            # cam_patch está en tamaño [feature_h, feature_w] (ej. 14x14 si\n",
    "            # la salida de la última conv es 14x14), la habíamos ampliado\n",
    "            # a 224 en la parte de `transform`? \n",
    "            #   -> Realmente NO, la red produce un feature map mas pequeño \n",
    "            #      (p.ej 14x14), y la transform es a la entrada no a la salida.\n",
    "            #\n",
    "            # Para superponer en la imagen \"patch\" (224x224) debemos:\n",
    "            # 1) reescalar 'cam_patch' a (224,224) -> \"cam_resized_224\"\n",
    "            # 2) luego, si queremos ponerlo en la ventana real (WINDOW_SIZE),\n",
    "            #    reescalamos 'cam_resized_224' a (WINDOW_SIZE, WINDOW_SIZE).\n",
    "            #\n",
    "            # Haremos la versión \"Grad-CAM local\" con respecto al parche de 224x224\n",
    "            # y luego lo reescalamos a la ventana real (WINDOW_SIZE, WINDOW_SIZE).\n",
    "            \n",
    "            cam_patch_torch = torch.from_numpy(cam_patch).unsqueeze(0).unsqueeze(0) \n",
    "            cam_resized_224 = F.interpolate(cam_patch_torch, size=(224,224), mode='bilinear', align_corners=False)\n",
    "            cam_resized_224 = cam_resized_224.squeeze().cpu().numpy()  # shape (224,224)\n",
    "            \n",
    "            # Ahora reescalar de (224,224) a (WINDOW_SIZE, WINDOW_SIZE)\n",
    "            cam_resized = cv2.resize(cam_resized_224, (WINDOW_SIZE, WINDOW_SIZE))\n",
    "            \n",
    "            #------------------------------------------------------------------\n",
    "            # (d) Acumular en gradcam_map\n",
    "            #------------------------------------------------------------------\n",
    "            gradcam_map[y : y + WINDOW_SIZE, x : x + WINDOW_SIZE] += cam_resized\n",
    "            count_map[y : y + WINDOW_SIZE, x : x + WINDOW_SIZE] += 1\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 8. Normalizar gradcam_map [0,1]\n",
    "    #--------------------------------------------------------------------------\n",
    "    count_map = np.maximum(count_map, 1e-5) \n",
    "    gradcam_map /= count_map\n",
    "    gc_min, gc_max = gradcam_map.min(), gradcam_map.max()\n",
    "    gradcam_map_norm = (gradcam_map - gc_min) / (gc_max - gc_min + 1e-8)\n",
    "\n",
    "    #--------------------------------------------------------------------------\n",
    "    # 9. Generar visualización superpuesta y guardarla\n",
    "    #--------------------------------------------------------------------------\n",
    "    base_img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    cmap = cm.get_cmap('jet')\n",
    "    heatmap_color = cmap(gradcam_map_norm)[..., :3]  # quitar canal alpha\n",
    "    heatmap_color = (heatmap_color * 255).astype(np.uint8)\n",
    "\n",
    "    alpha = 0.5\n",
    "    overlay = cv2.addWeighted(base_img_bgr, 1.0 - alpha, heatmap_color, alpha, 0)\n",
    "\n",
    "    # Guardar resultado\n",
    "    base_name, _ = os.path.splitext(os.path.basename(image_path))\n",
    "    output_filename = f\"{base_name}_gradcam.png\"\n",
    "    cv2.imwrite(output_filename, overlay)\n",
    "    print(f\"Grad-CAM guardado en: {output_filename}\")\n",
    "\n",
    "    return gradcam_map_norm\n",
    "\n",
    "\n",
    "# Ejemplo de uso:\n",
    "gradcam_map = generate_heatmap_gradcam(\"habana.jpg\", \"mejor_modelo_bordes.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASET ANOTATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk, ImageDraw\n",
    "import os\n",
    "\n",
    "# Configuración inicial\n",
    "input_folder = r\"E:\\Universidad\\ML\\ML-Project\\data\\Generales-Parciales\"\n",
    "output_folder = os.path.join(input_folder, \"annotations\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Variables globales\n",
    "image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "current_index = 0\n",
    "rect_start = None\n",
    "rect_end = None\n",
    "rect_id = None\n",
    "zoom_level = 1.0\n",
    "current_image = None\n",
    "img_tk = None\n",
    "offset_x = 0\n",
    "offset_y = 0\n",
    "pan_start = None\n",
    "\n",
    "# Funciones principales\n",
    "def load_image(index):\n",
    "    if 0 <= index < len(image_files):\n",
    "        image_path = os.path.join(input_folder, image_files[index])\n",
    "        image = Image.open(image_path)\n",
    "        return image\n",
    "    return None\n",
    "\n",
    "def save_rectangle(image, start, end, output_path):\n",
    "    cropped = image.crop((min(start[0], end[0]), min(start[1], end[1]),\n",
    "                          max(start[0], end[0]), max(start[1], end[1])))\n",
    "    cropped.save(output_path)\n",
    "\n",
    "def on_mouse_press(event):\n",
    "    global rect_start, rect_id, pan_start\n",
    "    if event.num == 1:  # Left click\n",
    "        rect_start = (int((event.x - offset_x) / zoom_level), int((event.y - offset_y) / zoom_level))\n",
    "        rect_id = canvas.create_rectangle(event.x, event.y, event.x, event.y, outline=\"red\")\n",
    "    elif event.num == 3:  # Right click\n",
    "        pan_start = (event.x, event.y)\n",
    "\n",
    "def on_mouse_drag(event):\n",
    "    global rect_id, offset_x, offset_y, pan_start\n",
    "    if rect_id and rect_start:\n",
    "        canvas.coords(rect_id, rect_start[0] * zoom_level + offset_x, rect_start[1] * zoom_level + offset_y, event.x, event.y)\n",
    "    elif pan_start:\n",
    "        dx = event.x - pan_start[0]\n",
    "        dy = event.y - pan_start[1]\n",
    "        offset_x += dx\n",
    "        offset_y += dy\n",
    "        pan_start = (event.x, event.y)\n",
    "        update_canvas()\n",
    "\n",
    "def on_mouse_release(event):\n",
    "    global rect_start, rect_end, rect_id, current_image, pan_start\n",
    "    if event.num == 1 and rect_start:  # Left click release\n",
    "        rect_end = (int((event.x - offset_x) / zoom_level), int((event.y - offset_y) / zoom_level))\n",
    "        if rect_start and rect_end:\n",
    "            output_path = os.path.join(output_folder, f\"annotation_{current_index}_{rect_start[0]}_{rect_start[1]}_{rect_end[0]}_{rect_end[1]}.png\")\n",
    "            save_rectangle(current_image, rect_start, rect_end, output_path)\n",
    "        rect_start = None\n",
    "        rect_end = None\n",
    "        rect_id = None\n",
    "    elif event.num == 3:  # Right click release\n",
    "        pan_start = None\n",
    "\n",
    "def on_mouse_wheel(event):\n",
    "    global zoom_level, offset_x, offset_y\n",
    "    factor = 1.1 if event.delta > 0 else 0.9\n",
    "    new_zoom_level = zoom_level * factor\n",
    "\n",
    "    # Adjust offsets to zoom relative to the cursor position\n",
    "    cursor_x = canvas.canvasx(event.x)\n",
    "    cursor_y = canvas.canvasy(event.y)\n",
    "    offset_x = cursor_x - factor * (cursor_x - offset_x)\n",
    "    offset_y = cursor_y - factor * (cursor_y - offset_y)\n",
    "\n",
    "    zoom_level = new_zoom_level\n",
    "    update_canvas()\n",
    "\n",
    "def next_image():\n",
    "    global current_index, current_image, img_tk, zoom_level, offset_x, offset_y\n",
    "    current_index += 1\n",
    "    zoom_level = 1.0\n",
    "    offset_x = 0\n",
    "    offset_y = 0\n",
    "    if current_index < len(image_files):\n",
    "        current_image = load_image(current_index)\n",
    "        update_canvas()\n",
    "    else:\n",
    "        print(\"No hay más imágenes.\")\n",
    "\n",
    "def update_canvas():\n",
    "    global img_tk\n",
    "    if current_image:\n",
    "        resized_image = current_image.resize((int(current_image.width * zoom_level), int(current_image.height * zoom_level)))\n",
    "        img_tk = ImageTk.PhotoImage(resized_image)\n",
    "        canvas.delete(\"all\")\n",
    "        canvas.config(scrollregion=(0, 0, resized_image.width + offset_x, resized_image.height + offset_y))\n",
    "        canvas.create_image(offset_x, offset_y, anchor=tk.NW, image=img_tk)\n",
    "\n",
    "# Crear la interfaz gráfica\n",
    "root = tk.Tk()\n",
    "root.title(\"Herramienta de Anotación Rápida\")\n",
    "\n",
    "canvas = tk.Canvas(root)\n",
    "canvas.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "frame_buttons = tk.Frame(root)\n",
    "frame_buttons.pack()\n",
    "\n",
    "btn_next = tk.Button(frame_buttons, text=\"Siguiente Imagen\", command=next_image)\n",
    "btn_next.pack(side=tk.LEFT)\n",
    "\n",
    "canvas.bind(\"<ButtonPress-1>\", on_mouse_press)\n",
    "canvas.bind(\"<B1-Motion>\", on_mouse_drag)\n",
    "canvas.bind(\"<ButtonRelease-1>\", on_mouse_release)\n",
    "canvas.bind(\"<MouseWheel>\", on_mouse_wheel)\n",
    "canvas.bind(\"<ButtonPress-3>\", on_mouse_press)\n",
    "canvas.bind(\"<B3-Motion>\", on_mouse_drag)\n",
    "canvas.bind(\"<ButtonRelease-3>\", on_mouse_release)\n",
    "\n",
    "# Cargar la primera imagen\n",
    "if image_files:\n",
    "    current_image = load_image(current_index)\n",
    "    update_canvas()\n",
    "else:\n",
    "    print(\"No se encontraron imágenes en la carpeta especificada.\")\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (128440320 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (111917568 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (106997760 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (106093056 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (106574208 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (106610944 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (106315008 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (105928832 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (117374976 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (102983680 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (114442560 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (113283648 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (154063936 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (156242944 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (116289920 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (115223040 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (95335680 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (95955200 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (133136960 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (95952896 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (100803328 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (96649216 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (97504704 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (116221440 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (133166080 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (113264256 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (114072000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (133329152 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (93490432 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (127777152 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (91280000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (126575488 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (125566272 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (124343296 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (115506112 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (91855360 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leona\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:3218: DecompressionBombWarning: Image size (110291648 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se generaron 5600 recortes aleatorios de 200x200 y se guardaron en la carpeta E:\\Universidad\\ML\\ML-Project\\data\\Generales-Parciales\\random_crops.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Configuración inicial\n",
    "input_folder = r\"E:\\Universidad\\ML\\ML-Project\\data\\Generales-Parciales\"\n",
    "output_folder = os.path.join(input_folder, \"random_crops\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Número total de subimágenes y tamaño del recorte\n",
    "num_total_crops = 5600\n",
    "crop_size = 200\n",
    "\n",
    "# Obtener la lista de imágenes\n",
    "image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "num_images = len(image_files)\n",
    "if num_images == 0:\n",
    "    raise ValueError(\"No se encontraron imágenes en la carpeta especificada.\")\n",
    "\n",
    "# Cantidad de recortes por imagen\n",
    "crops_per_image = num_total_crops // num_images\n",
    "\n",
    "# Generar recortes aleatorios\n",
    "def generate_random_crops(image_path, num_crops, crop_size, output_folder):\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "        for i in range(num_crops):\n",
    "            if width < crop_size or height < crop_size:\n",
    "                raise ValueError(f\"La imagen {image_path} es más pequeña que el tamaño del recorte ({crop_size}x{crop_size}).\")\n",
    "\n",
    "            left = random.randint(0, width - crop_size)\n",
    "            top = random.randint(0, height - crop_size)\n",
    "            right = left + crop_size\n",
    "            bottom = top + crop_size\n",
    "\n",
    "            crop = img.crop((left, top, right, bottom))\n",
    "            crop_filename = f\"{os.path.splitext(os.path.basename(image_path))[0]}_crop_{i}.png\"\n",
    "            crop.save(os.path.join(output_folder, crop_filename))\n",
    "\n",
    "# Procesar cada imagen\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(input_folder, image_file)\n",
    "    generate_random_crops(image_path, crops_per_image, crop_size, output_folder)\n",
    "\n",
    "print(f\"Se generaron {num_total_crops} recortes aleatorios de {crop_size}x{crop_size} y se guardaron en la carpeta {output_folder}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
